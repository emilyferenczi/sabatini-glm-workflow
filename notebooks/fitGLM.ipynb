{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This notebook assumes that you have already tuned the hyperparameters of the model using the gridSearch notebook, or, know the best hyperparameters to use for your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os \n",
    "if os.path.basename(os.getcwd()) == 'notebooks':\n",
    "    os.chdir('..')\n",
    "import glob\n",
    "\n",
    "from sglm import utils, glm_fit\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First, let's create a new project. The project directory will create a data and results folder and a config file.\n",
    "\n",
    "#### You will need to edit the config file with the particular glm params you wish to use. Fields that are necessary to edit are: predictors, predictors_shift_bounds, response, and the glm_keyword_args.\n",
    "\n",
    "#### You will also need to move your data into the data folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set data path\n",
    "workingDirectory = os.getcwd()\n",
    "dataFolder = 'projects'\n",
    "workingPath = os.path.join(workingDirectory, dataFolder)\n",
    "print(workingPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_name = 'AB120_combinedFirst_Lhemi_bestFit'\n",
    "project_dir = workingPath\n",
    "\n",
    "utils.create_new_project(project_name, project_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set analysis parameters in the config file\n",
    "predictor_list = ['go','nogo','lick_1R','lick_1','lick_2','lick_3','last_lick','lick_non1-3','missReward']\n",
    "#predictor_list = ['go','nogo','lick_1','lick_2','lick_3','lick_3R','lick_3C','last_lick','lick_non1-3','missReward','freeReward']\n",
    "predictors_shift_bounds_list = [-50, 100]\n",
    "response = 'photometryRhemi'\n",
    "regression_type = 'ElasticNet'\n",
    "alpha = 1e-05\n",
    "l1_ratio = 0.75"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import and Format Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input data should conform to the following convention and be saved as a *.csv:\n",
    "\n",
    "Indices / Unique Row Identifiers:\n",
    "* SessionName -- Any order is acceptable\n",
    "* TrialNumber-- Must be in chronological order, but does not need to start from zero\n",
    "* Timestamp -- Must be in chronological order, but does not need to start from zero\n",
    "\n",
    "Columns (Predictors + Responses):\n",
    "* Predictors - binary\n",
    "* Reponses - e.g. neural responses (analog or binary)\n",
    "\n",
    "Example, shown below is dummy data depicting a trial_0 that last four response timestamps:\n",
    "| SessionName | TrialNumber | Timestamp | predictor_1 | predictor_2 | predictor_3 | response_1 | response_2 |\n",
    "| --- | --- | --- | --- | --- | --- | --- | --- |\n",
    "| session_0 | trial_0 | -1 | 0 | 0 | 0 | 1 | 0.3 |\n",
    "| session_0 | trial_0 | 0 | 0 | 0 | 0 | 0 | 1.4 |\n",
    "| session_0 | trial_0 | 1 | 0 | 0 | 0 | 1 | 2.3 |\n",
    "| session_0 | trial_0 | 2 | 0 | 1 | 0 | 1 | 0.3 |\n",
    "| session_0 | trial_1 | -2 | 0 | 0 | 0 | 0 | 1.4 |\n",
    "| session_0 | trial_1 | -1 | 0 | 0 | 0 | 1 | 2.3 |\n",
    "| session_0 | trial_1 | 0 | 1 | 0 | 0 | 0 | 1.4 |\n",
    "| session_0 | trial_1 | 1 | 0 | 0 | 0 | 1 | 2.3 |\n",
    "| session_1 | trial_0 | 5 | 0 | 0 | 0 | 0 | 1.4 |\n",
    "| session_1 | trial_0 | 6 | 1 | 0 | 0 | 1 | 2.3 |\n",
    "| session_1 | trial_0 | 7 | 0 | 0 | 0 | 0 | 1.4 |\n",
    "| session_1 | trial_0 | 8 | 0 | 0 | 0 | 1 | 2.3 |\n",
    "| session_1 | trial_1 | 9 | 0 | 0 | 0 | 0 | 1.4 |\n",
    "| session_1 | trial_1 | 10 | 0 | 0 | 0 | 1 | 2.3 |\n",
    "...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, let's get set up to start our project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_path = os.path.join(project_dir, project_name)\n",
    "files = os.listdir(project_path)\n",
    "\n",
    "assert 'data' in files, 'data folder not found! {}'.format(files)\n",
    "assert 'results' in files, 'results folder not found! {}'.format(files)\n",
    "assert 'config.yaml' in files, 'config.yaml not found! {}'.format(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If needed, use the following function to combine multiple sessions into one csv. You will need a filename you wish to call your output_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_csv = 'combined.csv'\n",
    "\n",
    "utils.combine_csvs(project_path, output_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next, we'll load the data and set the columns you wish to use as fixed indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = os.path.join(project_path, 'data', output_csv)\n",
    "index_col = ['SessionName', 'TrialNumber', 'Timestamp']\n",
    "\n",
    "df = utils.read_data(input_file, index_col)\n",
    "\n",
    "print('Your dataframe has {} rows and {} columns'.format(df.shape[0], df.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### You can now explore and add to the dataframe. As an example, you may want to add various \"predictors\" or \"features\" to explore. You can use the example below as inspiration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Friendly reminder, the df we have imported is mutli-index, meaning, it's organization is dependent on 3-columns that we have set in index_col. Therefore, we can use \"groupby\" if you need to split the organization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define predictor labels\n",
    "df_source = predictor_labels.predictor_labels(df)\n",
    "df_source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confirm that the unique trials are equal to the expected numbers\n",
    "filtered_df = df_source[df_source['go']==1]\n",
    "unique_trial_numbers = pd.unique(filtered_df.index.get_level_values('TrialNumber'))\n",
    "unique_trial_numbers\n",
    "print(len(unique_trial_numbers))\n",
    "filtered_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load your fitting paramaters and set up your train/test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file = os.path.join(project_path, 'config.yaml')\n",
    "config = utils.load_config(config_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# edit the predictors and shift bounds in the config file to the ones that I would like\n",
    "config['glm_params']['predictors'] = predictor_list\n",
    "config['glm_params']['predictors_shift_bounds_default'] = predictors_shift_bounds_list\n",
    "config['glm_params']['response'] = response\n",
    "config['glm_params']['regression_type'] = regression_type\n",
    "config['glm_params']['glm_keyword_args']['elasticnet']['alpha'] = alpha\n",
    "config['glm_params']['glm_keyword_args']['elasticnet']['l1_ratio'] = l1_ratio\n",
    "\n",
    "# save back to config file\n",
    "cfg_file = os.path.join(project_dir, project_path, \"config.yaml\")\n",
    "utils.save_to_yaml(config, cfg_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shift responses and predictors. If you do not want to shift your predictors by an amount you set, feel free to comment out the entire \"predictors_shift_bounds\" in config.yaml. We will then use the default set when we created the config file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is the shift operation - remember to use df_source (that has the predictors, rather than df)\n",
    "response_shift, df_predictors_shift, shifted_params = glm_fit.shift_predictors(config, df_source)\n",
    "print('Your dataframe was shifted using: {}'.format(shifted_params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create your test/train datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test, y_train, y_test = glm_fit.split_data(df_predictors_shift, response_shift, config)\n",
    "\n",
    "print('Training data has {} rows and {} columns'.format(X_train.shape[0], X_train.shape[1]))\n",
    "print('Testing data has {} rows and {} columns'.format(X_test.shape[0], X_test.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, we're ready to run our GLM!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We have three different options: LinearRegression, ElasticNet, and Ridge.\n",
    "\n",
    "#### ElasticNet: This is a linear regression model trained with both L1 and L2 prior as regularizer. This combination allows for learning a sparse model where few of the weights are non-zero like Lasso, while still maintaining the regularization properties of Ridge. \n",
    "\n",
    "#### Ridge: This is a linear regression model trained with L2 prior as regularizer. This is the standard Linear Regression you're familiar with.\n",
    "\n",
    "#### LinearRegression: This is a standard linear regression model using the OLS method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### You can choose which model you would like to use by changing the \"regression_type\" field in the config.yaml file. You can also change the hyperparameters of the model in the glm_keyword_args field and run cross-validation to find the best hyperparameters. Just remember to change the relevant terms to lists in the config.yaml file. To run cross-validation, you can pass in the optional argument `cross_validatation=True` to the fit method and add `best_params` to your returned outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, y_pred, score, beta, intercept = glm_fit.fit_glm(config, X_train, X_test, y_train, y_test, cross_validation=False)\n",
    "print('Your model can account for {} percent of your data'.format(score*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save your outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create your model dictonary, this should include all the information you wish to save\n",
    "model_dict = {'model': model,\n",
    "              'model_type': config['glm_params']['regression_type'],\n",
    "              'y_pred': y_pred,\n",
    "              'score': score,\n",
    "              'beta': beta,\n",
    "              'intercept': intercept,\n",
    "              }\n",
    "\n",
    "#Save your model dictionary\n",
    "glm_fit.save_model(model_dict, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save model to text file\n",
    "model_save_path = config['Project']['project_path'] + '/results/model_results'\n",
    "with open(model_save_path, 'w') as f:\n",
    "    print(model_dict, file = f)\n",
    "\n",
    "model_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load your model dictionary if you already have it\n",
    "import pickle\n",
    "model_path = config['Project']['project_path'] + '/models'\n",
    "model_name = project_name + '_model.pkl'\n",
    "model_full_path = os.path.join(model_path, model_name)\n",
    "with open(model_full_path, 'rb') as f:\n",
    "    model_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_dict['model']\n",
    "beta = model_dict['beta']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate and save figures and results.\n",
    "The following requires non-sparse data. If you have sparse data, you will need to re-run `shift_predictors` with the `sparse` argument set to `False`.\n",
    "\n",
    "`plot_and_save` will save scatter plots of the predicted vs actual responses and the residuals and your beta coefficients. \n",
    "\n",
    "`plot_betas` will only *plot* the beta coefficients. \n",
    "\n",
    "`plot_aligned_dataStream` will plot the aligned data stream (e.g. aligned input data). You will need to run the `align_dataStream` function before running this plot.\n",
    "\n",
    "`plot_actual_v_reconstructed` will plot the actual vs reconstructed responses. You will need to run the `align_reconstructed_dataStream` function before running this plot.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first - you need to \"desparsify\" the data, in order to plot it - set sparsify back to false\n",
    "response_shift, df_predictors_shift, shifted_params = glm_fit.shift_predictors(config, df_source, sparsify=False)\n",
    "print('Your dataframe was shifted using: {}'.format(shifted_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test, y_train, y_test = glm_fit.split_data(df_predictors_shift, response_shift, config)\n",
    "\n",
    "print('Training data has {} rows and {} columns'.format(X_train.shape[0], X_train.shape[1]))\n",
    "print('Testing data has {} rows and {} columns'.format(X_test.shape[0], X_test.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glm_fit.plot_and_save(config, y_pred, y_test, beta, df_predictors_shift)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = os.path.join(project_path, 'results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "#retrieve predicted vs actual responses \n",
    "y_pred_all = model.predict(df_predictors_shift)\n",
    "y_actual = response_shift\n",
    "y_actual = y_actual.to_numpy()\n",
    "\n",
    "#plot y_actual and y_pred on same graph for an example section of data\n",
    "plt.plot(y_actual[13000:14000], color = 'b', lw = 0.5, label = 'actual data')\n",
    "plt.plot(y_pred_all[13000:14000], color = 'r', lw = 0.5, label = 'predicted data')\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.title(\"Overlay of actual data and GLM model predicted data\")\n",
    "plt.xlabel(\"Timestamps\")\n",
    "plt.ylabel(\"Response\")\n",
    "\n",
    "fig_save_path_png = os.path.join(project_dir, project_name, 'results\\\\overlay.png')\n",
    "fig_save_path_pdf = os.path.join(project_dir, project_name, 'results\\\\overlay.pdf')\n",
    "plt.savefig(fig_save_path_png)\n",
    "plt.savefig(fig_save_path_pdf)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "# Create new directory for beta plots\n",
    "save_dir = 'betas'\n",
    "parent_dir = config['Project']['project_path'] + '/results/'\n",
    "save_dir_path = os.path.join(parent_dir, save_dir)\n",
    "\n",
    "# Check if the directory already exists\n",
    "if not os.path.exists(save_dir_path):\n",
    "    # Create the directory\n",
    "    os.makedirs(save_dir_path)\n",
    "    print(\"Directory created successfully!\")\n",
    "else:\n",
    "    print(\"Directory already exists - will be overwritten!\")\n",
    "    #overwrite the directory\n",
    "    shutil.copytree(save_dir_path, save_dir_path, dirs_exist_ok = True)\n",
    "    \n",
    "save_path = save_dir_path\n",
    "\n",
    "utils.plot_betas(config, beta, df_predictors_shift, shifted_params, save=None, save_path=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Align the data and plot the actual and reconstructed responses (e.g. predicted y) against the true responses (e.g. neural responses) for each prediction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Align your actual data\n",
    "aligned_dataStream = utils.align_dataStream(config, df, shifted_params)\n",
    "\n",
    "# Create new directory for aligned data plots\n",
    "save_dir = 'aligned'\n",
    "parent_dir = config['Project']['project_path'] + '/results/'\n",
    "save_dir_path = os.path.join(parent_dir, save_dir)\n",
    "\n",
    "# Check if the directory already exists\n",
    "if not os.path.exists(save_dir_path):\n",
    "    # Create the directory\n",
    "    os.makedirs(save_dir_path)\n",
    "    print(\"Directory created successfully!\")\n",
    "else:\n",
    "    print(\"Directory already exists - will be overwritten!\")\n",
    "    #overwrite the directory\n",
    "    shutil.copytree(save_dir_path, save_dir_path, dirs_exist_ok = True)\n",
    "\n",
    "save_path = save_dir_path\n",
    "\n",
    "# Plot aligned data\n",
    "utils.plot_aligned_dataStream(aligned_dataStream, config, save=True, save_path=save_path, reconstructed=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruct your signal from your X-inputs\n",
    "recon_dataStream = utils.align_reconstructed_dataStream(config, \n",
    "                                                        df, df_predictors_shift,\n",
    "                                                         shifted_params, model)\n",
    "\n",
    "# Create new directory for reconstructed data plots\n",
    "save_dir = 'recon'\n",
    "parent_dir = config['Project']['project_path'] + '/results/'\n",
    "save_dir_path = os.path.join(parent_dir, save_dir)\n",
    "\n",
    "# Check if the directory already exists\n",
    "if not os.path.exists(save_dir_path):\n",
    "    # Create the directory\n",
    "    os.makedirs(save_dir_path)\n",
    "    print(\"Directory created successfully!\")\n",
    "else:\n",
    "    print(\"Directory already exists - will be overwritten!\")\n",
    "    #overwrite the directory\n",
    "    shutil.copytree(save_dir_path, save_dir_path, dirs_exist_ok = True)\n",
    "\n",
    "save_path = save_dir_path\n",
    "\n",
    "# Plot reconstructed data\n",
    "utils.plot_aligned_dataStream(recon_dataStream, config, save=True, save_path=save_path, reconstructed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new directory for actual vs reconstructed data plots\n",
    "save_dir = 'actualVrecon'\n",
    "parent_dir = config['Project']['project_path'] + '/results/'\n",
    "save_dir_path = os.path.join(parent_dir, save_dir)\n",
    "\n",
    "# Check if the directory already exists\n",
    "if not os.path.exists(save_dir_path):\n",
    "    # Create the directory\n",
    "    os.makedirs(save_dir_path)\n",
    "    print(\"Directory created successfully!\")\n",
    "else:\n",
    "    print(\"Directory already exists - will be overwritten!\")\n",
    "    #overwrite the directory\n",
    "    shutil.copytree(save_dir_path, save_dir_path, dirs_exist_ok = True)\n",
    "\n",
    "save_path = save_dir_path\n",
    "\n",
    "# Plot actual vs reconstructed\n",
    "utils.plot_actual_v_reconstructed(config, aligned_dataStream, recon_dataStream, save=True, save_path=save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional validation: \n",
    "\n",
    "In addition to using k-folds cross-validation, you can also use the `leave_one_out_cross_val` method to validate your model. This method will leave one predictor out and fit the model on the remaining predictors. A `model_list` will be returned with the `model` and `predictions` for each left out predictor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##NOTE: Cannot be sparse array for now\n",
    "model_list = glm_fit.leave_one_out_cross_val(config, X_train, X_test, y_train, y_test, plot=True)\n",
    "\n",
    "#Save your model_list\n",
    "import pickle\n",
    "LO_CV_path = (config['Project']['project_path'] + '/models/LO_CV_models.pkl')\n",
    "with open(LO_CV_path, 'wb') as f:\n",
    "        pickle.dump(model_list, f)\n",
    "\n",
    "        #save model_list to text file\n",
    "model_list_save_path = config['Project']['project_path'] + '/results/leave1Out_model_results'\n",
    "with open(model_list_save_path, 'w') as f:\n",
    "    print(model_list, file = f)\n",
    "\n",
    "model_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predictors temp - removing any reward\n",
    "#predictors_temp=['nogo', 'lick_3', 'lick_3C', 'last_lick', 'lick_non1-3']\n",
    "predictors_temp=['nogo', 'lick_1','last_lick', 'lick_non1-3']\n",
    "X_train_temp = X_train[predictors_temp]\n",
    "X_test_temp = X_test[predictors_temp]\n",
    "\n",
    "model_temp, y_pred, score, beta, intercept = glm_fit.fit_glm(config, X_train_temp, X_test_temp, y_train, y_test)\n",
    "\n",
    "#save model_list to text file\n",
    "model_temp_save_path = config['Project']['project_path'] + '/results/leaveRewardOut_model_results'\n",
    "with open(model_temp_save_path, 'w') as f:\n",
    "    print(score, file = f)\n",
    "\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validating train and test performance:\n",
    "The following can be used to assess train and test performance. \n",
    "This should be done after you have shifted the data, these cells are repeated below for clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_shift, df_predictors_shift, shifted_params = glm_fit.shift_predictors(config, df, sparsify=False)\n",
    "print('Your dataframe was shifted using: {}'.format(shifted_params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Below, you can fit the model with various train/test splits. This will *not* vary the test size, but instead, will vary the IDs used for training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create multiple train-test splits and fit the model\n",
    "#use AFTER you shift the data (NOT SPLIT) but BEFORE you fit the model\n",
    "\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "n_splits = 3 #set the number of splits you want here\n",
    "test_size = 0.2 #train test ratio.. this is NO longer pulling from config file\n",
    "shuffle_split = ShuffleSplit(n_splits=n_splits, test_size=test_size)\n",
    "\n",
    "X = df_predictors_shift\n",
    "y = response_shift\n",
    "\n",
    "#create train-test splits\n",
    "train_list = []\n",
    "test_list = []\n",
    "for i, (train_index, test_index) in enumerate(shuffle_split.split(X)):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    train_list.append((X_train, y_train))\n",
    "    test_list.append((X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run the model on each train-test split and plot\n",
    "results = []\n",
    "for i, (X_train, y_train) in enumerate(train_list):\n",
    "    print(f\"Fold {i+1}:\")\n",
    "    X_test, y_test = test_list[i]\n",
    "    model, y_pred, score, beta, intercept = glm_fit.fit_glm(config, X_train, X_test, y_train, y_test)\n",
    "    #calculate train score\n",
    "    #fetch regression type and score metric from config\n",
    "    regression_type = config['glm_params']['regression_type'].lower()\n",
    "    score_metric = config['glm_params']['glm_keyword_args'][regression_type]['score_metric']\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    if score_metric == 'r2':\n",
    "        train_score = model.score(X_train, y_train)\n",
    "    elif score_metric == 'mse':\n",
    "        train_score = glm_fit.calc_mse(y_train, y_train_pred)\n",
    "    print(f\"Train score: {train_score}\")\n",
    "    print(f\"Test score: {score}\")\n",
    "    results.append({'n_fold': i+1, 'Train_score': train_score, 'Test_score': score})\n",
    "\n",
    "#plot boxplot of results with confidence intervals\n",
    "import matplotlib.pyplot as plt\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.plot(kind='box', y=['Train_score', 'Test_score'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Below, you can fit the model with various train/test splits while doing the LOOCV analysis. This will *not* vary the test size, but instead, will vary the IDs used for training and testing. The results will be a dictionary with `n_folds`, `predictor_left_out`, and `scores`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shuffle and run the LOOCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run shuffle split cross validation on leave one out models\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "n_splits = 3 #set the number of splits you want here\n",
    "test_size = 0.2 #train test ratio.. this is NO longer pulling from config file\n",
    "shuffle_split = ShuffleSplit(n_splits=n_splits, test_size=test_size)\n",
    "\n",
    "X = df_predictors_shift\n",
    "y = response_shift\n",
    "\n",
    "#create train-test splits\n",
    "train_list = []\n",
    "test_list = []\n",
    "for i, (train_index, test_index) in enumerate(shuffle_split.split(X)):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    train_list.append((X_train, y_train))\n",
    "    test_list.append((X_test, y_test))\n",
    "\n",
    "#run the model on each train-test split\n",
    "results_LOOCV = []\n",
    "for i, (X_train, y_train) in enumerate(train_list):\n",
    "    print(f\"Fold {i+1}:\")\n",
    "    X_test, y_test = test_list[i]\n",
    "    model_list = glm_fit.leave_one_out_cross_val(config, X_train, X_test, y_train, y_test, plot=False)\n",
    "    results_LOOCV.append({'n_fold': i+1, 'models': model_list})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## FOR PLOTTING. WILL PARSE THE RESULTS AND PLOT THEM\n",
    "\n",
    "results_by_predictor = {}\n",
    "\n",
    "# Fetch test and train scores from each model in the model_list with fold number and predictor left out\n",
    "for i, result in enumerate(results_LOOCV):\n",
    "    print(f\"Processed fold {i+1}\")\n",
    "    model_list = result['models']\n",
    "    for j, model_dict in enumerate(model_list):\n",
    "        train_score = model_dict['train_score']\n",
    "        score = model_dict['test_score']\n",
    "        predictor_left_out = model_dict['predictor_left_out']\n",
    "        \n",
    "        # Check if the predictor_left_out already exists in the dictionary\n",
    "        if predictor_left_out not in results_by_predictor:\n",
    "            results_by_predictor[predictor_left_out] = {'predictor_left_out': predictor_left_out, 'Fold_scores': []}\n",
    "        \n",
    "        # Append the scores for the current fold to the corresponding predictor_left_out entry in the dictionary\n",
    "        results_by_predictor[predictor_left_out]['Fold_scores'].append({'n_fold': i+1, 'Test_score': score, 'Train_score': train_score})\n",
    "\n",
    "#make boxplot of results by predictor for train and test scores\n",
    "import seaborn as sns\n",
    "all_scores = []\n",
    "\n",
    "# Iterate through results_by_predictor to populate all_scores for easy plotting\n",
    "for key, value in results_by_predictor.items():\n",
    "    predictor = key\n",
    "    fold_scores = value['Fold_scores']\n",
    "    for fold_score in fold_scores:\n",
    "        train_score = fold_score['Train_score']\n",
    "        test_score = fold_score['Test_score']\n",
    "        all_scores.append({'Predictor_left_out': predictor, 'Score_type': 'Train_score', 'Score': train_score})\n",
    "        all_scores.append({'Predictor_left_out': predictor, 'Score_type': 'Test_score', 'Score': test_score})\n",
    "\n",
    "# Convert the list of dictionaries to a dataframe\n",
    "all_scores_df = pd.DataFrame(all_scores)\n",
    "\n",
    "# Plotting the box plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "boxplot = sns.boxplot(data=all_scores_df, x='Predictor_left_out', y='Score', hue='Score_type', palette='coolwarm')\n",
    "boxplot.set_title('Train and Test Scores by Predictor')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
